{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b351a9a-19d3-4aba-a129-361169fab54b",
   "metadata": {},
   "source": [
    "####   This script scrapes and collects information from a collection search page on the Smithsonian Institution's website, storing the data in Excel files. It includes a DataSheet class to handle Excel file creation, row addition, and saving, ensuring that headers are properly included. The get_info function extracts detailed information about individual items from their specific pages, parsing the HTML to gather attributes and their values. The MyThread function manages the scraping process by initializing an Excel sheet, iterating through search result pages, extracting item details, and updating the sheet. Finally, the script divides the total number of pages to be processed across multiple threads, facilitating concurrent data collection and organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc091bbf-2262-4856-aa26-0c77fcc3c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path\n",
    "import threading\n",
    "\n",
    "import openpyxl\n",
    "import requests\n",
    "from selenium.webdriver import Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sheetHeaders = [\"Name\", \"Provenance\", \"Collection\", \"Previous custodian or owner\", \"Origin\", \"Credit Line\", \"Type\",\n",
    "                \"Restrictions and Rights\", \"Period\", \"Geography\", \"Material\", \"Dimension\", \"EDAN ID\"]\n",
    "\n",
    "\n",
    "class DataSheet(object):\n",
    "  \n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "        if not os.path.exists(self.filename):\n",
    "            wb = openpyxl.Workbook()\n",
    "            wb.save(self.filename)\n",
    "        self.wb = openpyxl.load_workbook(self.filename)\n",
    "        self.sheet = self.wb.active\n",
    "       \n",
    "        global sheetHeaders\n",
    "        if self.sheet.max_row == 1:\n",
    "            self.sheet.append(sheetHeaders)\n",
    "\n",
    "    def add_row(self, data):\n",
    "        if self.sheet:\n",
    "            self.sheet.append(data)\n",
    "\n",
    "    def get_last_page(self):\n",
    "        if self.sheet:\n",
    "            global sheetHeaders\n",
    "            return self.sheet.cell(row=1, column=len(sheetHeaders) + 1).value\n",
    "        return None\n",
    "\n",
    "    def add_last_page(self, value):\n",
    "        if self.sheet:\n",
    "            global sheetHeaders\n",
    "            self.sheet.cell(row=1, column=len(sheetHeaders) + 1).value = str(value)\n",
    "\n",
    "    def save(self):\n",
    "        if self.wb:\n",
    "            self.wb.save(self.filename)\n",
    "\n",
    "\n",
    "def get_info(url):\n",
    "    \"\"\"\n",
    "    获取藏品详情\n",
    "    :param web:\n",
    "    :param url:\n",
    "    :return: 返回字典\n",
    "    \"\"\"\n",
    "    # web.get(url)\n",
    "    data = {}\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for li in html.find(\"div\", class_=\"individual-object-details\").find(\"ul\").find_all(\"li\"):\n",
    "            try:\n",
    "                head = li.find(\"h3\").text.strip()\n",
    "                content = \"\\n\".join([div.text.strip() for div in li.find(\"div\").find_all(\"div\")])\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                continue\n",
    "            data[str(head)] = content\n",
    "        for li in html.find(\"div\", class_=\"individual-object-at-a-glance__attributes\").find(\"ul\").find_all(\"li\"):\n",
    "            try:\n",
    "                head = li.find(\"h3\").text.strip()\n",
    "                content = li.find(\"div\").text.strip()\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                continue\n",
    "            data[str(head)] = content\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    return data\n",
    "\n",
    "\n",
    "def MyThread(start_page, num):\n",
    "    # 也可以用requests\n",
    "    # web = Chrome()\n",
    "    dataSheet = DataSheet(\"./search_%d.xlsx\" % start_page)\n",
    "    lastPage = dataSheet.get_last_page()\n",
    "    if lastPage is None or lastPage == \"\":\n",
    "        lastPage = start_page\n",
    "    else:\n",
    "        print(lastPage)\n",
    "        lastPage = int(lastPage)\n",
    "    for page in range(lastPage, (start_page + num)):\n",
    "        try:\n",
    "            url = f\"https://asia.si.edu/explore-art-culture/collections/search/?edan_q=Charles+Lang+Freer&listStart={page}\"\n",
    "            # web.get(url)\n",
    "            response = requests.get(url)\n",
    "            html = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \"search-results-image-grid__result\"\n",
    "            for div in html.find_all(\"div\", class_=\"search-results-image-grid__result\"):\n",
    "                try:\n",
    "                    img_url = div.find(\"img\", class_=\"search-results-image-grid__result-image\").get(\"src\")\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    img_url = None\n",
    "                a = div.find(\"a\", class_=\"secondary-link\")\n",
    "                name = a.text.strip().replace('\"', '')\n",
    "                sub_url = \"https://asia.si.edu\" + a.get(\"href\")\n",
    "                result = get_info(sub_url)\n",
    "                sheetData = [name]\n",
    "                global sheetHeaders\n",
    "                for i in range(1, len(sheetHeaders)):\n",
    "                    item = result.get(sheetHeaders[i], \"\")\n",
    "                    sheetData.append(item)\n",
    "                dataSheet.add_row(sheetData)\n",
    "            dataSheet.add_last_page(page)\n",
    "            dataSheet.save()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            continue\n",
    "    print(dataSheet.filename + \" finished!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    thread_num = 6\n",
    "    page_num = math.ceil(13590 / 12)\n",
    "    # page_num = 12\n",
    "    page_num_per_thread = math.ceil(page_num / thread_num)\n",
    "    for i in range(0, thread_num):\n",
    "        try:\n",
    "            xd = threading.Thread(target=MyThread, args=(page_num_per_thread * i, page_num_per_thread))\n",
    "            xd.start()  # 启动一个线程\n",
    "        except Exception as e:\n",
    "            print(\"run \" + str(e))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
